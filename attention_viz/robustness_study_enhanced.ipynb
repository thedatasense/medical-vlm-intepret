{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Medical VLM Robustness Study with Advanced Attention Techniques\n",
    "\n",
    "This notebook incorporates the suggested improvements for attention extraction and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate bitsandbytes scipy matplotlib opencv-python pillow torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import cv2\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Mount Google Drive if in Colab\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    sys.path.append('/content/drive/MyDrive/Robust_Medical_LLM_Dataset/attention_viz')\nexcept:\n    pass\n\n# Import enhanced modules with proper error handling\nimport sys\nimport os\n\n# Try using the helper first\ntry:\n    from colab_imports import *\n    IN_COLAB, paths = setup_colab_environment()\n    print(\"✓ Imported using colab_imports helper\")\nexcept ImportError:\n    # Manual import fallback\n    try:\n        from llava_rad_enhanced import (\n            EnhancedLLaVARadVisualizer, \n            AttentionConfig,\n            AttentionMetrics,\n            AttentionDifferenceAnalyzer\n        )\n        \n        from medgemma_enhanced import (\n            EnhancedAttentionExtractor,\n            AttentionExtractionConfig,\n            AttentionVisualizationEnhanced,\n            RobustAttentionAnalyzer\n        )\n        print(\"✓ Imported enhanced modules directly\")\n    except ImportError as e:\n        print(f\"❌ Import error: {e}\")\n        print(\"Make sure the enhanced modules are in your Python path\")\n        raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive if in Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    sys.path.append('/content/drive/MyDrive/Robust_Medical_LLM_Dataset/attention_viz')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Import enhanced modules\n",
    "from llava_rad_enhanced import (\n",
    "    EnhancedLLaVARadVisualizer, \n",
    "    AttentionConfig,\n",
    "    AttentionMetrics,\n",
    "    AttentionDifferenceAnalyzer\n",
    ")\n",
    "\n",
    "from medgemma_enhanced import (\n",
    "    EnhancedAttentionExtractor,\n",
    "    AttentionExtractionConfig,\n",
    "    AttentionVisualizationEnhanced,\n",
    "    RobustAttentionAnalyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced configuration with new parameters\n",
    "CONFIG = {\n",
    "    \"attention\": {\n",
    "        \"use_medical_colormap\": True,\n",
    "        \"multi_head_mode\": \"entropy_weighted\",  # New: entropy-weighted head aggregation\n",
    "        \"percentile_clip\": (5, 95),\n",
    "        \"use_body_mask\": True,\n",
    "        \"attention_head_reduction\": \"entropy_weighted\",\n",
    "        \"multi_token_aggregation\": \"weighted\",\n",
    "        \"fallback_chain\": [\"cross_attention\", \"gradcam\", \"uniform\"],\n",
    "        \"cache_enabled\": True\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"eval_limit\": 50,\n",
    "        \"n_variations_cap\": 8,\n",
    "        \"compute_multi_head\": True,\n",
    "        \"analyze_attention_shift\": True,\n",
    "        \"save_3d_surfaces\": False\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"base_csv\": \"medical-cxr-vqa-questions_sample.csv\",\n",
    "        \"var_csv\": \"medical-cxr-vqa-questions_sample_hardpositives.csv\",\n",
    "        \"image_root\": \"/content/drive/MyDrive/Robust_Medical_LLM_Dataset/MIMIC_JPG/hundred_vqa\",\n",
    "        \"output_dir\": \"outputs_enhanced\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"paths\"][\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['paths']['output_dir']}/attention_maps\", exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['paths']['output_dir']}/multi_head\", exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['paths']['output_dir']}/shift_analysis\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedModelWrapper:\n",
    "    \"\"\"Wrapper for models with enhanced attention extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str, config: Dict[str, Any]):\n",
    "        self.model_type = model_type\n",
    "        self.config = config\n",
    "        \n",
    "        if model_type == \"llava-rad\":\n",
    "            # Initialize enhanced LLaVA-Rad\n",
    "            attention_config = AttentionConfig(\n",
    "                use_medical_colormap=config[\"attention\"][\"use_medical_colormap\"],\n",
    "                multi_head_mode=config[\"attention\"][\"multi_head_mode\"],\n",
    "                percentile_clip=tuple(config[\"attention\"][\"percentile_clip\"])\n",
    "            )\n",
    "            self.visualizer = EnhancedLLaVARadVisualizer(\n",
    "                device=DEVICE,\n",
    "                config=attention_config\n",
    "            )\n",
    "            self.visualizer.load_model(load_in_8bit=True)\n",
    "            \n",
    "        elif model_type == \"medgemma\":\n",
    "            # Initialize enhanced MedGemma\n",
    "            from medgemma_launch_mimic_fixed import load_model_enhanced\n",
    "            \n",
    "            self.model, self.processor = load_model_enhanced(device=DEVICE)\n",
    "            \n",
    "            # Create enhanced extractor\n",
    "            extraction_config = AttentionExtractionConfig(\n",
    "                attention_head_reduction=config[\"attention\"][\"attention_head_reduction\"],\n",
    "                multi_token_aggregation=config[\"attention\"][\"multi_token_aggregation\"],\n",
    "                fallback_chain=config[\"attention\"][\"fallback_chain\"],\n",
    "                cache_enabled=config[\"attention\"][\"cache_enabled\"]\n",
    "            )\n",
    "            self.extractor = EnhancedAttentionExtractor(extraction_config)\n",
    "            self.analyzer = RobustAttentionAnalyzer(self.extractor)\n",
    "    \n",
    "    def extract_attention_with_answer(self, image: Image.Image, question: str, \n",
    "                                    keywords: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract attention and generate answer with enhanced techniques\"\"\"\n",
    "        \n",
    "        if self.model_type == \"llava-rad\":\n",
    "            result = self.visualizer.generate_with_attention(\n",
    "                image, question, max_new_tokens=50\n",
    "            )\n",
    "            \n",
    "            # Extract multi-head attention if available\n",
    "            multi_head = None\n",
    "            if (self.config[\"evaluation\"][\"compute_multi_head\"] and \n",
    "                isinstance(result.get('visual_attention'), list)):\n",
    "                multi_head = result['visual_attention']\n",
    "            \n",
    "            return {\n",
    "                \"answer\": result[\"answer\"],\n",
    "                \"attention_map\": result.get(\"visual_attention\"),\n",
    "                \"method\": result.get(\"attention_method\", \"unknown\"),\n",
    "                \"metrics\": result.get(\"metrics\", {}),\n",
    "                \"multi_head_attention\": multi_head\n",
    "            }\n",
    "            \n",
    "        else:  # medgemma\n",
    "            # Generate with attention\n",
    "            inputs = self.processor(\n",
    "                text=f\"<image>{question}\",\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                gen_result = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    output_attentions=True,\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "            \n",
    "            # Extract attention with enhanced method\n",
    "            attention_grid, token_indices, method = self.extractor.extract_token_conditioned_attention_robust(\n",
    "                self.model, self.processor, gen_result, keywords, image, question\n",
    "            )\n",
    "            \n",
    "            # Decode answer\n",
    "            answer = self.processor.tokenizer.decode(\n",
    "                gen_result.sequences[0], skip_special_tokens=True\n",
    "            ).split(\"Assistant:\")[-1].strip()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = AttentionMetrics.calculate_focus_score(\n",
    "                attention_grid,\n",
    "                roi_mask=self._get_body_mask(image) if self.config[\"attention\"][\"use_body_mask\"] else None\n",
    "            )\n",
    "            metrics[\"sparsity\"] = AttentionMetrics.calculate_sparsity(attention_grid)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"attention_map\": attention_grid,\n",
    "                \"method\": method,\n",
    "                \"metrics\": metrics,\n",
    "                \"token_indices\": token_indices\n",
    "            }\n",
    "    \n",
    "    def _get_body_mask(self, image: Image.Image) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get body mask for medical image\"\"\"\n",
    "        try:\n",
    "            from medgemma_enhanced import create_tight_body_mask\n",
    "            gray = np.array(image.convert('L'))\n",
    "            return create_tight_body_mask(gray)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "print(\"Initializing enhanced models...\")\n",
    "llava_wrapper = EnhancedModelWrapper(\"llava-rad\", CONFIG)\n",
    "medgemma_wrapper = EnhancedModelWrapper(\"medgemma\", CONFIG)\n",
    "print(\"✓ Models initialized with enhanced attention extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_case_enhanced(case_data: Dict, \n",
    "                         llava_wrapper: EnhancedModelWrapper,\n",
    "                         medgemma_wrapper: EnhancedModelWrapper) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a single case with enhanced attention analysis\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(case_data[\"image_path\"]).convert(\"RGB\")\n",
    "    base_question = case_data[\"base_q\"]\n",
    "    variations = case_data[\"variations\"][:CONFIG[\"evaluation\"][\"n_variations_cap\"]]\n",
    "    \n",
    "    # Extract keywords for attention gating\n",
    "    keywords = extract_medical_keywords(base_question)\n",
    "    \n",
    "    results = {\"study_id\": case_data[\"study_id\"], \"models\": {}}\n",
    "    \n",
    "    for model_name, wrapper in [(\"llava-rad\", llava_wrapper), (\"medgemma\", medgemma_wrapper)]:\n",
    "        print(f\"\\nProcessing {model_name} for case {case_data['study_id']}...\")\n",
    "        \n",
    "        # Base question results\n",
    "        base_result = wrapper.extract_attention_with_answer(image, base_question, keywords)\n",
    "        \n",
    "        # Variation results\n",
    "        var_results = []\n",
    "        attention_maps = [base_result[\"attention_map\"]]\n",
    "        \n",
    "        for var_q in variations:\n",
    "            var_keywords = extract_medical_keywords(var_q)\n",
    "            var_result = wrapper.extract_attention_with_answer(image, var_q, var_keywords)\n",
    "            var_results.append(var_result)\n",
    "            \n",
    "            if var_result[\"attention_map\"] is not None:\n",
    "                attention_maps.append(var_result[\"attention_map\"])\n",
    "        \n",
    "        # Calculate enhanced metrics\n",
    "        consistency_score = AttentionMetrics.calculate_consistency(attention_maps)\n",
    "        \n",
    "        # Calculate attention shifts if requested\n",
    "        shift_analyses = []\n",
    "        if CONFIG[\"evaluation\"][\"analyze_attention_shift\"] and len(attention_maps) > 1:\n",
    "            for i, (var_q, var_att) in enumerate(zip(variations, attention_maps[1:])):\n",
    "                if base_result[\"attention_map\"] is not None and var_att is not None:\n",
    "                    shift = AttentionDifferenceAnalyzer.compute_attention_shift(\n",
    "                        base_result[\"attention_map\"], var_att\n",
    "                    )\n",
    "                    shift_analyses.append({\n",
    "                        \"variation\": var_q,\n",
    "                        \"total_shift\": shift[\"total_shift\"],\n",
    "                        \"js_divergence\": shift[\"js_divergence\"],\n",
    "                        \"com_shift\": shift[\"center_of_mass_shift\"]\n",
    "                    })\n",
    "        \n",
    "        # Save visualizations\n",
    "        save_enhanced_visualizations(\n",
    "            case_data[\"study_id\"], model_name, image, \n",
    "            base_result, var_results, shift_analyses\n",
    "        )\n",
    "        \n",
    "        # Compile results\n",
    "        results[\"models\"][model_name] = {\n",
    "            \"base_answer\": base_result[\"answer\"],\n",
    "            \"base_metrics\": base_result[\"metrics\"],\n",
    "            \"attention_method\": base_result[\"method\"],\n",
    "            \"consistency_score\": float(consistency_score),\n",
    "            \"variation_results\": [\n",
    "                {\"answer\": vr[\"answer\"], \"metrics\": vr[\"metrics\"]} \n",
    "                for vr in var_results\n",
    "            ],\n",
    "            \"shift_analyses\": shift_analyses,\n",
    "            \"has_multi_head\": base_result.get(\"multi_head_attention\") is not None\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_enhanced_visualizations(study_id: str, model_name: str, image: Image.Image,\n",
    "                               base_result: Dict, var_results: List[Dict],\n",
    "                               shift_analyses: List[Dict]):\n",
    "    \"\"\"Save enhanced visualizations including multi-head and shift analysis\"\"\"\n",
    "    \n",
    "    output_base = f\"{CONFIG['paths']['output_dir']}/attention_maps/{study_id}_{model_name}\"\n",
    "    \n",
    "    # Save base attention overlay\n",
    "    if base_result[\"attention_map\"] is not None:\n",
    "        overlay = AttentionVisualizationEnhanced.create_attention_overlay(\n",
    "            image, base_result[\"attention_map\"],\n",
    "            use_body_mask=CONFIG[\"attention\"][\"use_body_mask\"]\n",
    "        )\n",
    "        overlay.save(f\"{output_base}_base.png\")\n",
    "    \n",
    "    # Save multi-head visualization if available\n",
    "    if base_result.get(\"multi_head_attention\") is not None:\n",
    "        if model_name == \"llava-rad\":\n",
    "            fig = llava_wrapper.visualizer.create_multi_head_visualization(\n",
    "                base_result[\"multi_head_attention\"], image,\n",
    "                save_path=f\"{CONFIG['paths']['output_dir']}/multi_head/{study_id}_multi_head.png\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Save 3D surface if requested\n",
    "    if CONFIG[\"evaluation\"][\"save_3d_surfaces\"] and base_result[\"attention_map\"] is not None:\n",
    "        if model_name == \"llava-rad\":\n",
    "            fig = llava_wrapper.visualizer.create_3d_attention_surface(\n",
    "                base_result[\"attention_map\"],\n",
    "                save_path=f\"{output_base}_3d_surface.png\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Save shift analysis visualization\n",
    "    if shift_analyses and model_name == \"llava-rad\":\n",
    "        # Create comparison figure for largest shift\n",
    "        max_shift = max(shift_analyses, key=lambda x: x[\"total_shift\"])\n",
    "        max_shift_idx = shift_analyses.index(max_shift)\n",
    "        \n",
    "        fig = llava_wrapper.visualizer.visualize_attention_difference(\n",
    "            image, \n",
    "            case_data[\"base_q\"],\n",
    "            max_shift[\"variation\"],\n",
    "            save_path=f\"{CONFIG['paths']['output_dir']}/shift_analysis/{study_id}_max_shift.png\"\n",
    "        )\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def extract_medical_keywords(question: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Extract medical keywords for attention gating\"\"\"\n",
    "    medical_terms = {\n",
    "        \"effusion\", \"pneumonia\", \"consolidation\", \"edema\", \"atelectasis\",\n",
    "        \"nodule\", \"mass\", \"cardiomegaly\", \"pleural\", \"pneumothorax\",\n",
    "        \"opacity\", \"infiltrate\", \"fracture\", \"emphysema\", \"fibrosis\"\n",
    "    }\n",
    "    \n",
    "    words = [w.strip(\",.?;:\").lower() for w in question.split()]\n",
    "    keywords = [w for w in words if w in medical_terms]\n",
    "    \n",
    "    # Add common diagnostic terms if no medical terms found\n",
    "    if not keywords:\n",
    "        if \"normal\" in question.lower():\n",
    "            keywords = [\"normal\"]\n",
    "        elif \"abnormal\" in question.lower():\n",
    "            keywords = [\"abnormal\"]\n",
    "        else:\n",
    "            keywords = words[:1]  # First content word\n",
    "    \n",
    "    return keywords[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Enhanced Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from robustness_study_notebook import RobustPromptDataset\n",
    "\n",
    "dataset = RobustPromptDataset(\n",
    "    CONFIG[\"paths\"][\"base_csv\"],\n",
    "    CONFIG[\"paths\"][\"var_csv\"],\n",
    "    CONFIG[\"paths\"][\"image_root\"]\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} cases\")\n",
    "\n",
    "# Run evaluation\n",
    "all_results = []\n",
    "limit = min(CONFIG[\"evaluation\"][\"eval_limit\"], len(dataset))\n",
    "\n",
    "print(f\"\\nEvaluating {limit} cases with enhanced attention techniques...\")\n",
    "\n",
    "for idx in range(limit):\n",
    "    try:\n",
    "        case_data = dataset.get(idx)\n",
    "        if case_data[\"image_path\"] is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Case {idx+1}/{limit}: {case_data['study_id']}\")\n",
    "        \n",
    "        results = evaluate_case_enhanced(case_data, llava_wrapper, medgemma_wrapper)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        with open(f\"{CONFIG['paths']['output_dir']}/results_enhanced.json\", \"w\") as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing case {idx}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n✓ Enhanced evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Enhanced Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results into dataframe\n",
    "rows = []\n",
    "for result in all_results:\n",
    "    study_id = result[\"study_id\"]\n",
    "    \n",
    "    for model_name, model_data in result[\"models\"].items():\n",
    "        # Calculate answer consistency\n",
    "        base_ans = extract_yes_no(model_data[\"base_answer\"])\n",
    "        var_answers = [extract_yes_no(vr[\"answer\"]) for vr in model_data[\"variation_results\"]]\n",
    "        consistency = sum(1 for va in var_answers if va == base_ans) / len(var_answers) if var_answers else 1.0\n",
    "        \n",
    "        # Calculate mean attention shift\n",
    "        shift_scores = [sa[\"js_divergence\"] for sa in model_data[\"shift_analyses\"]]\n",
    "        mean_shift = np.mean(shift_scores) if shift_scores else 0.0\n",
    "        \n",
    "        row = {\n",
    "            \"study_id\": study_id,\n",
    "            \"model\": model_name,\n",
    "            \"attention_method\": model_data[\"attention_method\"],\n",
    "            \"answer_consistency\": consistency,\n",
    "            \"attention_consistency\": model_data[\"consistency_score\"],\n",
    "            \"mean_js_divergence\": mean_shift,\n",
    "            \"focus_score\": model_data[\"base_metrics\"].get(\"focus\", 0),\n",
    "            \"roi_focus\": model_data[\"base_metrics\"].get(\"roi_focus\", 0),\n",
    "            \"sparsity\": model_data[\"base_metrics\"].get(\"sparsity\", 0),\n",
    "            \"has_multi_head\": model_data[\"has_multi_head\"],\n",
    "            \"vulnerability\": mean_shift * (1 - consistency)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "\n",
    "def extract_yes_no(text: str) -> str:\n",
    "    \"\"\"Extract yes/no answer from text\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    if text.startswith(\"yes\") or \" yes\" in text[:20]:\n",
    "        return \"yes\"\n",
    "    elif text.startswith(\"no\") or \" no\" in text[:20]:\n",
    "        return \"no\"\n",
    "    return \"uncertain\"\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nEnhanced Evaluation Results Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in df_results[\"model\"].unique():\n",
    "    model_df = df_results[df_results[\"model\"] == model]\n",
    "    \n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    print(f\"  Attention Methods Used: {model_df['attention_method'].value_counts().to_dict()}\")\n",
    "    print(f\"  Answer Consistency: {model_df['answer_consistency'].mean():.3f} ± {model_df['answer_consistency'].std():.3f}\")\n",
    "    print(f\"  Attention Consistency: {model_df['attention_consistency'].mean():.3f} ± {model_df['attention_consistency'].std():.3f}\")\n",
    "    print(f\"  Mean JS Divergence: {model_df['mean_js_divergence'].mean():.3f} ± {model_df['mean_js_divergence'].std():.3f}\")\n",
    "    print(f\"  Focus Score: {model_df['focus_score'].mean():.3f} ± {model_df['focus_score'].std():.3f}\")\n",
    "    print(f\"  ROI Focus: {model_df['roi_focus'].mean():.3f} ± {model_df['roi_focus'].std():.3f}\")\n",
    "    print(f\"  Sparsity (Gini): {model_df['sparsity'].mean():.3f} ± {model_df['sparsity'].std():.3f}\")\n",
    "    print(f\"  Vulnerability Score: {model_df['vulnerability'].mean():.3f} ± {model_df['vulnerability'].std():.3f}\")\n",
    "    print(f\"  Multi-head Available: {model_df['has_multi_head'].sum()} / {len(model_df)} cases\")\n",
    "\n",
    "# Save detailed results\n",
    "df_results.to_csv(f\"{CONFIG['paths']['output_dir']}/enhanced_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Enhanced Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of enhanced metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    (\"attention_consistency\", \"Attention Consistency Score\"),\n",
    "    (\"focus_score\", \"Attention Focus Score\"),\n",
    "    (\"roi_focus\", \"ROI Focus Ratio\"),\n",
    "    (\"sparsity\", \"Attention Sparsity (Gini)\"),\n",
    "    (\"mean_js_divergence\", \"Mean JS Divergence\"),\n",
    "    (\"vulnerability\", \"Vulnerability Score\")\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Create box plots for each model\n",
    "    data_to_plot = []\n",
    "    labels = []\n",
    "    \n",
    "    for model in [\"medgemma\", \"llava-rad\"]:\n",
    "        model_data = df_results[df_results[\"model\"] == model][metric]\n",
    "        data_to_plot.append(model_data)\n",
    "        labels.append(model.title())\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = ['#4CAF50', '#2196F3']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_ylabel(\"Value\", fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean values\n",
    "    for i, (model, color) in enumerate(zip([\"medgemma\", \"llava-rad\"], colors)):\n",
    "        mean_val = df_results[df_results[\"model\"] == model][metric].mean()\n",
    "        ax.axhline(y=mean_val, xmin=i*0.5+0.1, xmax=i*0.5+0.4, \n",
    "                  color=color, linestyle='--', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['paths']['output_dir']}/enhanced_metrics_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Create attention method distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, model in enumerate([\"medgemma\", \"llava-rad\"]):\n",
    "    ax = axes[idx]\n",
    "    model_df = df_results[df_results[\"model\"] == model]\n",
    "    \n",
    "    method_counts = model_df[\"attention_method\"].value_counts()\n",
    "    ax.pie(method_counts.values, labels=method_counts.index, autopct='%1.1f%%')\n",
    "    ax.set_title(f\"{model.title()} - Attention Methods Used\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['paths']['output_dir']}/attention_method_distribution.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Enhanced Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = {\n",
    "    \"study_info\": {\n",
    "        \"title\": \"Enhanced Medical VLM Robustness Study with Advanced Attention Techniques\",\n",
    "        \"n_cases\": len(all_results),\n",
    "        \"n_variations_per_case\": CONFIG[\"evaluation\"][\"n_variations_cap\"],\n",
    "        \"attention_config\": CONFIG[\"attention\"]\n",
    "    },\n",
    "    \"model_performance\": {},\n",
    "    \"key_findings\": {}\n",
    "}\n",
    "\n",
    "for model in [\"medgemma\", \"llava-rad\"]:\n",
    "    model_df = df_results[df_results[\"model\"] == model]\n",
    "    \n",
    "    report[\"model_performance\"][model] = {\n",
    "        \"attention_methods\": model_df[\"attention_method\"].value_counts().to_dict(),\n",
    "        \"metrics\": {\n",
    "            \"answer_consistency\": {\n",
    "                \"mean\": float(model_df[\"answer_consistency\"].mean()),\n",
    "                \"std\": float(model_df[\"answer_consistency\"].std()),\n",
    "                \"min\": float(model_df[\"answer_consistency\"].min()),\n",
    "                \"max\": float(model_df[\"answer_consistency\"].max())\n",
    "            },\n",
    "            \"attention_consistency\": {\n",
    "                \"mean\": float(model_df[\"attention_consistency\"].mean()),\n",
    "                \"std\": float(model_df[\"attention_consistency\"].std())\n",
    "            },\n",
    "            \"focus_metrics\": {\n",
    "                \"focus_score\": float(model_df[\"focus_score\"].mean()),\n",
    "                \"roi_focus\": float(model_df[\"roi_focus\"].mean()),\n",
    "                \"sparsity\": float(model_df[\"sparsity\"].mean())\n",
    "            },\n",
    "            \"robustness\": {\n",
    "                \"mean_js_divergence\": float(model_df[\"mean_js_divergence\"].mean()),\n",
    "                \"vulnerability\": float(model_df[\"vulnerability\"].mean())\n",
    "            }\n",
    "        },\n",
    "        \"multi_head_support\": {\n",
    "            \"available\": int(model_df[\"has_multi_head\"].sum()),\n",
    "            \"percentage\": float(model_df[\"has_multi_head\"].mean() * 100)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Key findings\n",
    "report[\"key_findings\"] = {\n",
    "    \"most_robust_model\": \"medgemma\" if df_results.groupby(\"model\")[\"vulnerability\"].mean()[\"medgemma\"] < df_results.groupby(\"model\")[\"vulnerability\"].mean()[\"llava-rad\"] else \"llava-rad\",\n",
    "    \"attention_extraction_success_rate\": {\n",
    "        model: float((df_results[df_results[\"model\"] == model][\"attention_method\"] != \"uniform\").mean() * 100)\n",
    "        for model in [\"medgemma\", \"llava-rad\"]\n",
    "    },\n",
    "    \"average_attention_shift\": float(df_results[\"mean_js_divergence\"].mean()),\n",
    "    \"correlation_answer_attention_consistency\": float(\n",
    "        df_results[[\"answer_consistency\", \"attention_consistency\"]].corr().iloc[0, 1]\n",
    "    )\n",
    "}\n",
    "\n",
    "# Save report\n",
    "with open(f\"{CONFIG['paths']['output_dir']}/enhanced_robustness_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nEnhanced Robustness Report Generated!\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(report[\"key_findings\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "del llava_wrapper\n",
    "del medgemma_wrapper\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"✓ Enhanced robustness study complete!\")\n",
    "print(f\"\\nResults saved to: {CONFIG['paths']['output_dir']}/\")\n",
    "print(\"\\nKey outputs:\")\n",
    "print(\"- enhanced_results.csv: Detailed metrics for all cases\")\n",
    "print(\"- enhanced_robustness_report.json: Summary report\")\n",
    "print(\"- attention_maps/: Enhanced attention visualizations\")\n",
    "print(\"- multi_head/: Multi-head attention visualizations\")\n",
    "print(\"- shift_analysis/: Attention shift visualizations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}