{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA-Rad Medical Setup for Google Colab\n",
    "This notebook sets up LLaVA-Rad and MedGemma for medical image analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install all dependencies including open_clip\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q open_clip_torch timm einops\n",
    "!pip install -q transformers>=4.36.0 accelerate bitsandbytes\n",
    "!pip install -q opencv-python scipy matplotlib pillow\n",
    "!pip install -q tokenizers sentencepiece protobuf gradio\n",
    "\n",
    "print(\"✅ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Clone repositories with conflict prevention\nimport os\nimport sys\n\n# Set environment variable to prevent auto-registration\nos.environ['TRANSFORMERS_OFFLINE'] = '0'\nos.environ['HF_HUB_OFFLINE'] = '0'\n\n# Clone LLaVA-Rad\nif not os.path.exists('/content/LLaVA-Rad'):\n    !git clone https://github.com/microsoft/LLaVA-Rad.git /content/LLaVA-Rad\n\n# Clone medical VLM repo\nif not os.path.exists('/content/medical-vlm-intepret'):\n    !git clone https://github.com/thedatasense/medical-vlm-intepret.git /content/medical-vlm-intepret\n\n# Modify LLaVA to prevent conflicts before installing\nimport subprocess\n\n# Create a setup script that modifies LLaVA before install\nsetup_script = '''\nimport os\nimport sys\n\n# Fix the __init__.py file to prevent conflicts\ninit_file = '/content/LLaVA-Rad/llava/__init__.py'\nif os.path.exists(init_file):\n    with open(init_file, 'r') as f:\n        content = f.read()\n    \n    # Add conflict fix at the beginning\n    fix_code = \"\"\"\n# Prevent transformers conflict\nimport warnings\nwarnings.filterwarnings('ignore', message=\".*already used by a Transformers config.*\")\n\ntry:\n    import transformers.models.auto.configuration_auto as cfg\n    if hasattr(cfg.CONFIG_MAPPING, '_extra_content'):\n        cfg.CONFIG_MAPPING._extra_content.pop('llava', None)\n        cfg.CONFIG_MAPPING._extra_content.pop('llava_next', None)\nexcept:\n    pass\n\n\"\"\"\n    \n    if 'Prevent transformers conflict' not in content:\n        with open(init_file, 'w') as f:\n            f.write(fix_code + content)\n        print(\"✓ Modified LLaVA __init__.py to prevent conflicts\")\n'''\n\n# Write and run the setup script\nwith open('/content/fix_llava.py', 'w') as f:\n    f.write(setup_script)\n\n!python /content/fix_llava.py\n\n# Now install LLaVA-Rad\n%cd /content/LLaVA-Rad\n!pip install -e . --quiet\n\n# Add to Python path\nsys.path.insert(0, '/content/LLaVA-Rad')\nsys.path.insert(0, '/content/medical-vlm-intepret/attention_viz')\n\nprint(\"✅ Repositories cloned and installed with conflict prevention\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 2.5: Alternative - Use HuggingFace LLaVA if conflicts persist\n# Run this cell ONLY if you get \"'llava' is already used\" error\n\nprint(\"Using HuggingFace LLaVA models as alternative...\")\n\n# Skip LLaVA-Rad installation and use HF models directly\nimport sys\nsys.path.insert(0, '/content/medical-vlm-intepret/attention_viz')\n\n# Create a compatibility layer\ncompatibility_code = '''\n# llava_compat.py - Compatibility layer for HF models\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nimport torch\n\nclass LLaVACompat:\n    @staticmethod\n    def load_pretrained_model(model_path, model_base, model_name, load_8bit, load_4bit, device):\n        \"\"\"Compatibility wrapper for HF models\"\"\"\n        \n        # Map model paths\n        hf_model_map = {\n            \"microsoft/llava-med-v1.5-mistral-7b\": \"llava-hf/llava-1.5-7b-hf\",\n            \"liuhaotian/llava-v1.5-7b\": \"llava-hf/llava-1.5-7b-hf\"\n        }\n        \n        hf_path = hf_model_map.get(model_path, \"llava-hf/llava-1.5-7b-hf\")\n        print(f\"Using HuggingFace model: {hf_path}\")\n        \n        # Load processor and model\n        processor = AutoProcessor.from_pretrained(hf_path)\n        \n        # Quantization config\n        kwargs = {\n            \"device_map\": device if device == \"auto\" else {\"\": device},\n            \"torch_dtype\": torch.float16\n        }\n        \n        if load_8bit:\n            from transformers import BitsAndBytesConfig\n            kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n                load_in_8bit=True,\n                llm_int8_enable_fp32_cpu_offload=True\n            )\n        \n        model = LlavaForConditionalGeneration.from_pretrained(hf_path, **kwargs)\n        \n        return processor.tokenizer, model, processor.image_processor, 2048\n\n# Mock the llava module\nimport types\nllava = types.ModuleType(\"llava\")\nllava.model = types.ModuleType(\"llava.model\")\nllava.model.builder = types.ModuleType(\"llava.model.builder\")\nllava.model.builder.load_pretrained_model = LLaVACompat.load_pretrained_model\n\nllava.utils = types.ModuleType(\"llava.utils\")\nllava.utils.disable_torch_init = lambda: None\n\n# Add to sys.modules\nsys.modules[\"llava\"] = llava\nsys.modules[\"llava.model\"] = llava.model\nsys.modules[\"llava.model.builder\"] = llava.model.builder\nsys.modules[\"llava.utils\"] = llava.utils\n'''\n\n# Write compatibility layer\nwith open('/content/llava_compat.py', 'w') as f:\n    f.write(compatibility_code)\n\n# Import it\nexec(compatibility_code)\n\nprint(\"✅ HuggingFace compatibility layer created\")\nprint(\"You can now proceed with the rest of the notebook\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Fix conflicts before importing LLaVA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Pre-emptively fix conflicts before any LLaVA imports\ndef fix_conflicts_before_import():\n    try:\n        # Fix transformers conflicts\n        import transformers.models.llava\n        import transformers.models.auto.configuration_auto as configuration_auto\n        \n        # Remove from config mapping\n        if hasattr(configuration_auto.CONFIG_MAPPING, '_extra_content'):\n            extra = configuration_auto.CONFIG_MAPPING._extra_content\n            for model in ['llava', 'llava_next']:\n                if model in extra:\n                    del extra[model]\n                    print(f\"✓ Removed {model} from CONFIG_MAPPING\")\n        \n        # Also fix in model mappings\n        import transformers.models.auto.modeling_auto as modeling_auto\n        for attr in dir(modeling_auto):\n            if attr.endswith('_MAPPING'):\n                mapping = getattr(modeling_auto, attr)\n                if hasattr(mapping, '_extra_content') and 'llava' in mapping._extra_content:\n                    del mapping._extra_content['llava']\n                    print(f\"✓ Removed llava from {attr}\")\n    except Exception as e:\n        print(f\"Pre-import fix: {e}\")\n\n# Apply fixes\nfix_conflicts_before_import()\n\n# Now test imports\nprint(\"\\nTesting LLaVA imports...\")\ntry:\n    # Import LLaVA components\n    from llava.model.builder import load_pretrained_model\n    from llava.utils import disable_torch_init\n    from llava.conversation import conv_templates\n    from llava.mm_utils import process_images, tokenizer_image_token\n    from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n    print(\"✅ LLaVA-Rad imports successful\")\nexcept ImportError as e:\n    print(f\"❌ Import error: {e}\")\n    print(\"\\nTrying alternative import method...\")\n    # Try to import just the essentials\n    try:\n        import llava\n        print(f\"✓ llava module found at: {llava.__file__}\")\n        from llava.model.builder import load_pretrained_model\n        from llava.utils import disable_torch_init\n        print(\"✅ Essential imports successful\")\n    except Exception as e2:\n        print(f\"❌ Alternative import also failed: {e2}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load LLaVA-Rad Medical Model\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load model function\n",
    "def load_llava_medical():\n",
    "    from llava.model.builder import load_pretrained_model\n",
    "    from llava.utils import disable_torch_init\n",
    "    from llava.mm_utils import get_model_name_from_path\n",
    "    \n",
    "    disable_torch_init()\n",
    "    \n",
    "    # Try medical model first, then fallback\n",
    "    model_paths = [\n",
    "        \"microsoft/llava-med-v1.5-mistral-7b\",\n",
    "        \"liuhaotian/llava-v1.5-7b\"\n",
    "    ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        try:\n",
    "            print(f\"Loading {model_path}...\")\n",
    "            tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "                model_path=model_path,\n",
    "                model_base=None,\n",
    "                model_name=get_model_name_from_path(model_path),\n",
    "                load_8bit=False,\n",
    "                load_4bit=False,\n",
    "                device=\"cuda\"\n",
    "            )\n",
    "            model.eval()\n",
    "            print(f\"✅ Loaded {model_path}\")\n",
    "            return tokenizer, model, image_processor\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    raise RuntimeError(\"Could not load any model\")\n",
    "\n",
    "# Load the model\n",
    "tokenizer, model, image_processor = load_llava_medical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Mount Drive and Test\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for medical images\n",
    "data_root = '/content/drive/MyDrive/Robust_Medical_LLM_Dataset'\n",
    "image_dir = f'{data_root}/MIMIC_JPG/hundred_vqa'\n",
    "\n",
    "if os.path.exists(image_dir):\n",
    "    images = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    if images:\n",
    "        test_image_path = os.path.join(image_dir, images[0])\n",
    "        print(f\"✅ Found {len(images)} medical images\")\n",
    "        \n",
    "        # Test the model\n",
    "        image = Image.open(test_image_path).convert('RGB')\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Test Medical Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate answer\n",
    "        from llava.conversation import conv_templates, SeparatorStyle\n",
    "        from llava.mm_utils import process_images, tokenizer_image_token\n",
    "        from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "        \n",
    "        conv = conv_templates[\"llava_v1\"].copy()\n",
    "        question = \"Is there evidence of pneumonia in this chest X-ray?\"\n",
    "        \n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + '\\n' + question\n",
    "        else:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + question\n",
    "        \n",
    "        conv.append_message(conv.roles[0], prompt)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt_text = conv.get_prompt()\n",
    "        \n",
    "        input_ids = tokenizer_image_token(\n",
    "            prompt_text, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n",
    "        ).unsqueeze(0).cuda()\n",
    "        \n",
    "        image_tensor = process_images([image], image_processor, model.config)[0]\n",
    "        image_tensor = image_tensor.unsqueeze(0).half().cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                images=image_tensor,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=100,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        if conv.sep_style == SeparatorStyle.TWO:\n",
    "            answer = outputs.split(conv.sep2)[-1].strip()\n",
    "        else:\n",
    "            answer = outputs.split(conv.sep)[-1].strip()\n",
    "        \n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "else:\n",
    "    print(\"❌ Medical images not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load MedGemma\n",
    "from medgemma_enhanced import load_medgemma, build_inputs, generate_answer\n",
    "\n",
    "print(\"Loading MedGemma...\")\n",
    "medgemma_model, medgemma_processor = load_medgemma(\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"✅ MedGemma loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Use the Medical-Only Module\n",
    "from llava_rad_medical_only import LLaVARadMedical, MedicalAttentionConfig\n",
    "\n",
    "# Create medical model wrapper\n",
    "config = MedicalAttentionConfig(\n",
    "    colormap='hot',\n",
    "    attention_head_mode='mean',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "llava_medical = LLaVARadMedical(config=config)\n",
    "\n",
    "# Use already loaded model\n",
    "llava_medical.model = model\n",
    "llava_medical.tokenizer = tokenizer\n",
    "llava_medical.image_processor = image_processor\n",
    "\n",
    "print(\"✅ Medical wrapper ready\")\n",
    "\n",
    "# Test with attention extraction\n",
    "if 'test_image_path' in locals():\n",
    "    result = llava_medical.generate_with_attention(\n",
    "        test_image_path,\n",
    "        \"What are the main findings in this chest X-ray?\",\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['answer']}\")\n",
    "    print(f\"Attention method: {result.get('attention_method', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run Full Analysis\n",
    "%cd /content/medical-vlm-intepret/attention_viz\n",
    "!python run_medical_vlm_analysis.py --n_studies 5 --output_dir /content/results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}